"""
Global novelty sweep on top of EASE.

This script fixes the EASE lambda_reg and varies a global novelty
weight lambda_nov in a post-processing reranking step:

  final_score(u, i) = s_ease(u, i) + lambda_nov * novelty(u, i)

Where novelty(u, i) is the average genre distance between candidate
item i and the items in user u's history.

For each novelty lambda it computes accuracy, novelty, and diversity
metrics and saves the results as a CSV.
"""

from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.data.loader import load_interactions, load_games
from src.evaluation.splitter import split_train_in_out
from src.evaluation.evaluator import evaluate_model
from src.models.ease import EASE
from src.novelty.rerank import rerank_with_novelty
from src.config import LAMBDA_REG, TOP_K, N_EVAL_USERS, SEED, POP_ALPHA


# --------------------------------------------------
# Helpers to load precomputed genre matrices
# --------------------------------------------------
def load_genre_similarity(path: str = "../data/processed/genre_similarity.npy"):
    """
    Load precomputed item item genre similarity matrix.
    """
    return np.load(path)


def load_genre_distance(path: str = "../data/processed/genre_distance.npy"):
    """
    Load precomputed item item genre distance matrix.
    """
    return np.load(path)


# --------------------------------------------------
# Main sweep
# --------------------------------------------------
def run_novelty_sweep(
    ease_lambda_reg: float = LAMBDA_REG,
    novelty_lambdas = None,
    top_k_eval: int = TOP_K,
    top_k_base: int = 100,
    n_eval_users: int = N_EVAL_USERS,
    seed: int = SEED,
    output_csv: str = "novelty_sweep_genre.csv",
) -> pd.DataFrame:
    """
    Run a sweep over global novelty weights on top of a fixed EASE model.

    Steps
    -----
      1) Load full interactions and games.
      2) Split interactions into train_in / train_out (one holdout per user).
      3) Sample up to n_eval_users users for offline evaluation.
      4) Train EASE once on train_in and generate a base recommendation
         list per user of length top_k_base.
      5) For each novelty lambda:
           - rerank the base list with rerank_with_novelty
           - evaluate accuracy and novelty metrics with evaluate_model
      6) Save one CSV with all configurations.

    Parameters
    ----------
    ease_lambda_reg : float, default LAMBDA_REG
        Regularization for the underlying EASE model.
    novelty_lambdas : list or None
        Global novelty weights to evaluate. If None, a default range
        is used.
    top_k_eval : int, default TOP_K
        Cutoff k for evaluation metrics.
    top_k_base : int, default 100
        Length of the candidate list generated by EASE before
        reranking.
    n_eval_users : int, default N_EVAL_USERS
        Maximum number of users to include in the offline evaluation.
    seed : int, default SEED
        Random seed for user sampling and splitting.
    output_csv : str, default "novelty_sweep_genre.csv"
        File name for the output CSV under notebooks/results/sweeps.

    Returns
    -------
    pd.DataFrame
        One row per configuration with all metrics.
    """
    if novelty_lambdas is None:
        # include 0 (no novelty) plus a range of positive weights
        novelty_lambdas = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0]

    rng = np.random.default_rng(seed)

    # 1) Load data
    train = load_interactions(train=True)
    games = load_games()
    # maps item_id -> publisher (needed for publisher Gini)
    publisher_mapper = games.set_index("item_id")["publisher"]

    # 2) Codabench style split: 1 holdout per user
    train_in_full, train_out_full = split_train_in_out(train, seed=seed)

    # users that actually have a holdout
    all_users = train_out_full["user_id"].unique()
    n_eval = min(n_eval_users, len(all_users))
    sample_users = rng.choice(all_users, size=n_eval, replace=False)

    train_in = (
        train_in_full[train_in_full["user_id"].isin(sample_users)]
        .reset_index(drop=True)
    )
    train_out = (
        train_out_full[train_out_full["user_id"].isin(sample_users)]
        .reset_index(drop=True)
    )

    print(
        f"[INFO] Novelty sweep on {n_eval} users "
        f"(train_in rows = {len(train_in)}, train_out rows = {len(train_out)})"
    )

    # 3) Load precomputed genre matrices
    item_similarity = load_genre_similarity()
    item_distance = load_genre_distance()

    # 4) Train EASE once and get base recommendations
    model = EASE(lambda_reg=ease_lambda_reg, alpha_pop=POP_ALPHA)
    recs_base = model.recommend(train_in, train_in, top_k=top_k_base)

    # 5) Evaluate plain EASE (no novelty) as lambda_nov = 0 baseline
    print("[INFO] Evaluating baseline (lambda_nov = 0)")
    metrics_base = evaluate_model(
        recs_base,
        train_in,
        train_out,
        publisher_mapper=publisher_mapper,
        item_similarity=item_similarity,
        item_distance=item_distance,
        k=top_k_eval,
    )
    metrics_base["ease_lambda_reg"] = ease_lambda_reg
    metrics_base["novelty_lambda"] = 0.0

    results = [metrics_base]

    # 6) Sweep novelty weights
    for lam in tqdm(novelty_lambdas, desc="Sweeping novelty lambda"):
        if lam == 0.0:
            # already done
            continue

        # rerank
        recs_novel = rerank_with_novelty(
            recs_base,
            train_in,
            item_distance,
            lambda_val=lam,
        )

        # sort by user, then score descending
        recs_novel = recs_novel.sort_values(
            ["user_id", "score"],
            ascending=[True, False],
        ).reset_index(drop=True)

        metrics = evaluate_model(
            recs_novel,
            train_in,
            train_out,
            publisher_mapper=publisher_mapper,
            item_similarity=item_similarity,
            item_distance=item_distance,
            k=top_k_eval,
        )

        metrics["ease_lambda_reg"] = ease_lambda_reg
        metrics["novelty_lambda"] = lam

        results.append(metrics)

    root_dir = Path(__file__).resolve().parents[2]
    output_dir = root_dir / "notebooks" / "results" / "sweeps"
    output_dir.mkdir(parents=True, exist_ok=True)

    df = pd.DataFrame(results)
    output_path = output_dir / output_csv
    df.to_csv(output_path, index=False)
    print(f"[INFO] Saved novelty sweep submissions to: {output_path}")

    return df
