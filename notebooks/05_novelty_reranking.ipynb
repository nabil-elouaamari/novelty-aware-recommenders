{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NOVELTY RERANKING (GLOBAL GENRE-BASED DISTANCE)",
   "id": "dbc0666ff4f6279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:12:56.517116919Z",
     "start_time": "2025-12-16T01:12:56.452562128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from src.experiments.novelty_sweep import run_novelty_sweep"
   ],
   "id": "61f7c3a31ae5e0f5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:15:44.318146154Z",
     "start_time": "2025-12-16T01:12:56.955067910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_nov = run_novelty_sweep(output_csv=\"novelty_sweep_genre_offline.csv\")\n",
    "df_nov"
   ],
   "id": "66baa542546bc257",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Novelty sweep on 4000 users (train_in rows = 176308, train_out rows = 4000)\n",
      "[INFO] Evaluating baseline (lambda_nov = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sweeping novelty λ: 100%|██████████| 8/8 [00:47<00:00,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved novelty sweep results to: /home/sunaj/Desktop/novelty-aware-recommenders/results/sweeps/novelty_sweep_genre_offline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       ndcg   recall  user_coverage  item_gini  publisher_gini  item_coverage  \\\n",
       "0  0.230748  0.41975            1.0   0.804540        0.802155       0.070536   \n",
       "1  0.165827  0.31275            1.0   0.784548        0.796837       0.102209   \n",
       "2  0.177077  0.33100            1.0   0.786574        0.794599       0.098442   \n",
       "3  0.190688  0.35400            1.0   0.789566        0.796120       0.092964   \n",
       "4  0.231134  0.41975            1.0   0.804535        0.803569       0.073275   \n",
       "5  0.228261  0.41600            1.0   0.802283        0.795972       0.068995   \n",
       "6  0.220888  0.40475            1.0   0.798153        0.791832       0.068139   \n",
       "7  0.206378  0.38000            1.0   0.790601        0.789508       0.066598   \n",
       "\n",
       "   intra_list_similarity   novelty  ease_lambda_reg  novelty_lambda  \n",
       "0               0.456687  0.557470              300            0.00  \n",
       "1               0.742240  0.426371              300           -2.00  \n",
       "2               0.724135  0.431452              300           -1.50  \n",
       "3               0.694407  0.440897              300           -1.00  \n",
       "4               0.491911  0.533304              300           -0.05  \n",
       "5               0.412338  0.591514              300            0.05  \n",
       "6               0.374597  0.623312              300            0.10  \n",
       "7               0.313551  0.675101              300            0.20  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg</th>\n",
       "      <th>recall</th>\n",
       "      <th>user_coverage</th>\n",
       "      <th>item_gini</th>\n",
       "      <th>publisher_gini</th>\n",
       "      <th>item_coverage</th>\n",
       "      <th>intra_list_similarity</th>\n",
       "      <th>novelty</th>\n",
       "      <th>ease_lambda_reg</th>\n",
       "      <th>novelty_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230748</td>\n",
       "      <td>0.41975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.802155</td>\n",
       "      <td>0.070536</td>\n",
       "      <td>0.456687</td>\n",
       "      <td>0.557470</td>\n",
       "      <td>300</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165827</td>\n",
       "      <td>0.31275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784548</td>\n",
       "      <td>0.796837</td>\n",
       "      <td>0.102209</td>\n",
       "      <td>0.742240</td>\n",
       "      <td>0.426371</td>\n",
       "      <td>300</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177077</td>\n",
       "      <td>0.33100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.786574</td>\n",
       "      <td>0.794599</td>\n",
       "      <td>0.098442</td>\n",
       "      <td>0.724135</td>\n",
       "      <td>0.431452</td>\n",
       "      <td>300</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.35400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789566</td>\n",
       "      <td>0.796120</td>\n",
       "      <td>0.092964</td>\n",
       "      <td>0.694407</td>\n",
       "      <td>0.440897</td>\n",
       "      <td>300</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.41975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.804535</td>\n",
       "      <td>0.803569</td>\n",
       "      <td>0.073275</td>\n",
       "      <td>0.491911</td>\n",
       "      <td>0.533304</td>\n",
       "      <td>300</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.41600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802283</td>\n",
       "      <td>0.795972</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>0.412338</td>\n",
       "      <td>0.591514</td>\n",
       "      <td>300</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.220888</td>\n",
       "      <td>0.40475</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798153</td>\n",
       "      <td>0.791832</td>\n",
       "      <td>0.068139</td>\n",
       "      <td>0.374597</td>\n",
       "      <td>0.623312</td>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.206378</td>\n",
       "      <td>0.38000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.790601</td>\n",
       "      <td>0.789508</td>\n",
       "      <td>0.066598</td>\n",
       "      <td>0.313551</td>\n",
       "      <td>0.675101</td>\n",
       "      <td>300</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"../results/sweeps/novelty_sweep_genre.csv\")\n",
    "\n",
    "# sort by novelty_lambda to get smooth curves\n",
    "df = df.sort_values(\"novelty_lambda\")\n",
    "\n",
    "# 1) Lambda vs metrics (one plot with 4 lines)\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for metric in [\"ndcg\", \"recall\", \"novelty\"]:\n",
    "    plt.plot(df[\"novelty_lambda\"], df[metric], marker=\"o\", label=metric)\n",
    "\n",
    "plt.xlabel(\"novelty_lambda\")\n",
    "plt.ylabel(\"metric value\")\n",
    "plt.title(\"Metrics vs novelty_lambda\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "7b75fd36536cdf52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = df[\"novelty\"]\n",
    "y = df[\"ndcg\"]\n",
    "lam = df[\"novelty_lambda\"]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x, y, label=\"All configs\", alpha=0.6)\n",
    "\n",
    "# draw Pareto curve\n",
    "pareto = df.sort_values(\"novelty\")\n",
    "plt.plot(pareto[\"novelty\"], pareto[\"ndcg\"], \"r-o\", label=\"Pareto front\")\n",
    "\n",
    "# annotate each point with λ\n",
    "for xv, yv, lv in zip(x, y, lam):\n",
    "    plt.annotate(\n",
    "        f\"{lv:.2f}\",\n",
    "        (xv, yv),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(3, 3),\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel(\"Novelty\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "plt.title(\"Accuracy(ndcg)–novelty tradeoff\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "847ed2de9296724d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Conclusion\n",
    "- I fixed the EASE regularization at λ_reg = 300 and reranked its top 100 candidates per user using a global genre-based novelty weight `novelty_lambda`.\n",
    "- increasing `novelty_lambda` increases novelty metric (from about 0.55 to 0.85) and slightly improves item coverage.\n",
    "- At the same time, increasing `novelty_lambda` decreases accuracy (NDCG drops from about 0.228 to 0.075).\n",
    "- The accuracy–novelty Pareto curve is strictly decreasing and smooth, so each value of novelty_lambda corresponds to a different compromise."
   ],
   "id": "5957a435315248ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SAVE A NOVELTY-RERANKED SUBMISSION FOR CODABENCH",
   "id": "c8739b28fb90d27c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.data.loader import load_interactions, load_games\n",
    "from src.models.ease import EASE\n",
    "from src.novelty.rerank import rerank_with_novelty\n",
    "from src.pipelines.save import save_submission\n",
    "import numpy as np\n",
    "from src.config import LAMBDA_REG\n",
    "\n",
    "# 1. Load data\n",
    "train = load_interactions(train=True)\n",
    "test_in = load_interactions(train=False)\n",
    "games = load_games()\n",
    "\n",
    "# 2. Load precomputed genre distance\n",
    "item_distance = np.load(\"../data/processed/genre_distance.npy\")\n",
    "\n",
    "# 3. Train EASE with fixed lambda_reg\n",
    "model = EASE(lambda_reg=float(LAMBDA_REG))\n",
    "\n",
    "# recommend() already uses a Codabench style mapping\n",
    "recs_base = model.recommend(train, test_in, top_k=100)"
   ],
   "id": "aff209ca2e9b4eba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Pick a novelty weight, for example, 0.3\n",
    "NOVELTY_LAMBDA = 0.0\n",
    "\n",
    "# re-rank scores\n",
    "recs_novel = rerank_with_novelty(\n",
    "    recs_base,\n",
    "    test_in,\n",
    "    item_distance,\n",
    "    lambda_val=NOVELTY_LAMBDA,\n",
    ")\n",
    "\n",
    "# keep only top-20 per user, sorted by score\n",
    "recs_novel = (\n",
    "    recs_novel\n",
    "    .sort_values([\"user_id\", \"score\"], ascending=[True, False])\n",
    "    .groupby(\"user_id\")\n",
    "    .head(20)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# drop scores for submission\n",
    "submission = recs_novel[[\"user_id\", \"item_id\"]]\n",
    "\n",
    "# 5. Save for Codabench\n",
    "save_submission(submission, f\"ease_lambda{LAMBDA_REG}_nov_{NOVELTY_LAMBDA}\")"
   ],
   "id": "8253775af7b9881d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NOVELTY RERANKING (USER-SPECIFIC GENRE DISTANCE)",
   "id": "862526de0ad0f8bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.experiments.user_specific_sweep import run_user_specific_sweep\n",
    "\n",
    "# Profile-size-based weighting\n",
    "metrics_profile = run_user_specific_sweep(grouping=\"profile\")\n",
    "metrics_profile"
   ],
   "id": "93ea807029ea5a60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Genre-diversity-based weighting\n",
    "metrics_genre = run_user_specific_sweep(grouping=\"genre\")\n",
    "metrics_genre"
   ],
   "id": "84aa4a92bfea98ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combined strategy\n",
    "metrics_combined = run_user_specific_sweep(grouping=\"combined\")\n",
    "metrics_combined"
   ],
   "id": "2666c8e0f8427b89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global_df = pd.read_csv(\"../results/sweeps/novelty_sweep_genre.csv\")\n",
    "\n",
    "profile_df = pd.read_csv(\"../results/sweeps/user_specific_profile.csv\")\n",
    "genre_df   = pd.read_csv(\"../results/sweeps/user_specific_genre.csv\")\n",
    "combo_df   = pd.read_csv(\"../results/sweeps/user_specific_combined.csv\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(global_df[\"novelty\"], global_df[\"ndcg\"], label=\"Global novelty\", alpha=0.7)\n",
    "plt.scatter(profile_df[\"novelty\"], profile_df[\"ndcg\"], label=\"Profile-based novelty\", s=80)\n",
    "plt.scatter(genre_df[\"novelty\"], genre_df[\"ndcg\"], label=\"Genre-diversity novelty\", s=80)\n",
    "plt.scatter(combo_df[\"novelty\"], combo_df[\"ndcg\"], label=\"Combined\", s=80)\n",
    "\n",
    "plt.xlabel(\"Novelty\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "plt.title(\"Accuracy–Novelty Tradeoff (User-specific vs Global)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "86314cb6f83ea584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How did we group users?\n",
    "<b> A) Profile Size Grouping (Interactions per User):</b>\n",
    "- Users with few interactions have very uncertain distance estimates, too little history to judge novelty → low λ.\n",
    "- Users with many interactions have a stable, well-defined taste profile → novelty becomes meaningful → higher λ.\n",
    "\n",
    "- Distribution:\n",
    "    - mean: 42 items\n",
    "    - median: 26\n",
    "    - 25% < 10\n",
    "    - 50% < 26\n",
    "    - 75% < 55\n",
    "\n",
    "| Profile size | Rationale                          | Novelty weight λ |\n",
    "| ------------ | ---------------------------------- | ---------------- |\n",
    "| < 10 items   | sparse history, novelty unstable   | **0.05**         |\n",
    "| 10–55 items  | majority of users, stable baseline | **0.20**         |\n",
    "| > 55 items   | rich profiles, novelty meaningful  | **0.50**         |\n",
    "\n",
    "<b> B) Genre Diversity Grouping (Entropy) </b>\n",
    "- Entropy is a measure of how diverse or spread out the genres in a user's history are.\n",
    "    - A user who only plays Action has: `entropy ≈ 0`\n",
    "    - A user who plays Action + RPG + Strategy + Indie has: `entropy ≈ higher`\n",
    "- Intuition:\n",
    "    - Low entropy → niche taste → novelty may hurt accuracy\n",
    "    - High entropy → broad taste → novelty may help\n",
    "    - Medium entropy → balanced\n",
    "\n",
    "| Genre Entropy | Meaning         | λ        |\n",
    "| ------------- | --------------- | -------- |\n",
    "| low           | niche taste     | **0.05** |\n",
    "| medium        | mixed           | **0.20** |\n",
    "| high          | very open taste | **0.50** |\n",
    "\n",
    "<b> C) Combined Grouping </b>\n",
    "- You simply take the average λ from profile-size and genre-diversity groups.\n",
    "- This creates a hybrid behavior:\n",
    "    - Users with large profiles and high genre entropy → highest novelty\n",
    "    - Users with small profiles and low entropy → minimal novelty\n",
    "    - Mixed cases → in between"
   ],
   "id": "75b48777be5d3fe0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why all user-specific results ended up so close\n",
    "<b>Reason 1—The majority of users fall in the middle groups</b>\n",
    "\n",
    "Most users have:\n",
    "* profile size = 10–55\n",
    "* entropy = mid-range\n",
    "\n",
    "→ They all received λ = 0.20.\n",
    "\n",
    "<b>Reason 2—Low and high groups were small</b>\n",
    "\n",
    "Very few users received λ = 0.05 or λ = 0.50.\n",
    "\n",
    "Since only 3000 users were sampled in the sweep, the differences shrink even further.\n",
    "\n",
    "<b>Reason 3—EASE scores dominate the ranking</b>\n",
    "\n",
    "EASE provides strong baseline scores, so novelty adjustments with moderate λ values (0.05–0.5) produce only small ranking changes.\n",
    "\n",
    "<b>`s' = s + λ*N`</b>"
   ],
   "id": "5761d76912882d90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion: Effect of User-Specific Novelty Weighting\n",
    "- We tested three user-specific novelty weighting strategies, based on interaction profile size, genre diversity (entropy), and a hybrid of both, and compared them to the global novelty baseline.\n",
    "- Across all three strategies, the resulting novelty values clustered around 0.66, with NDCG scores between 0.214 and 0.219. These points lie in the same region as the global novelty sweep with λ ≈ 0.1–0.2, which represents the best accuracy–novelty balance for fixed global values.\n",
    "- <b>The main result is that user-specific novelty weighting does not significantly outperform global novelty weighting.</b>\n",
    "    - Instead, it reproduces the upper-left portion of the global accuracy–novelty trade-off curve without providing a more favorable trade-off.\n",
    "- This limited improvement is likely due to:\n",
    "    - Most users have similar interaction counts, causing many of them to receive the same novelty weight.\n",
    "    - Genre diversity (entropy) is also being concentrated in a narrow range, again reducing differentiation between users.\n",
    "    - EASE scores are dominant, so novelty adjustments with moderate λ values (0.05–0.5) produce only small ranking changes.\n",
    "\n",
    "| Method          | Novelty | NDCG        | Comment                                     |\n",
    "| --------------- | ------- | ----------- | ------------------------------------------- |\n",
    "| Profile size    | 0.659   | **0.21925** | Slightly lower novelty, reasonable accuracy |\n",
    "| Genre diversity | 0.6655  | 0.21489     | Close to profile, a bit worse               |\n",
    "| Combined        | 0.6663  | 0.21752     | In between                                  |\n"
   ],
   "id": "136f8b72c9ec436b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SAVE A NOVELTY-RERANKED SUBMISSION FOR CODABENCH",
   "id": "4520adef89c9bd7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.experiments.user_specific_sweep import user_specific_rerank\n",
    "from src.data.loader import load_interactions, load_games\n",
    "from src.novelty.user_groups import (\n",
    "    assign_groups_by_profile_size,\n",
    "    assign_groups_by_genre_diversity,\n",
    "    assign_groups_combined,\n",
    ")\n",
    "import numpy as np\n",
    "from src.config import LAMBDA_REG\n",
    "from src.models.ease import EASE\n",
    "from src.pipelines.save import save_submission\n",
    "\n",
    "# 1. Load data\n",
    "train = load_interactions(train=True)\n",
    "test_in = load_interactions(train=False)\n",
    "games = load_games()\n",
    "\n",
    "item_distance = np.load(\"../data/processed/genre_distance.npy\")\n",
    "\n",
    "# 3. Train EASE with fixed lambda_reg\n",
    "model = EASE(lambda_reg=float(LAMBDA_REG))\n",
    "\n",
    "# recommend() already uses a Codabench style mapping\n",
    "recs_grouping = model.recommend(train, test_in, top_k=300)\n",
    "\n",
    "# user-specific lambda per user, computed from the fold in interactions (test_in)\n",
    "lambda_profile  = assign_groups_by_profile_size(test_in)\n",
    "lambda_genre    = assign_groups_by_genre_diversity(test_in, games)\n",
    "lambda_combined = assign_groups_combined(test_in, games)\n",
    "\n",
    "def build_and_save_user_specific_submission(recs, test_in, item_distance, user_lambda, grouping):\n",
    "    recs_us = user_specific_rerank(\n",
    "        recs,\n",
    "        test_in,\n",
    "        item_distance,\n",
    "        user_lambda,\n",
    "    )\n",
    "\n",
    "    recs_us = (\n",
    "        recs_us\n",
    "        .sort_values([\"user_id\", \"score\"], ascending=[True, False])\n",
    "        .groupby(\"user_id\")\n",
    "        .head(20)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    submission = recs_us[[\"user_id\", \"item_id\"]]\n",
    "    save_submission(submission, f\"ease_lambda{LAMBDA_REG}_usnov_{grouping}\")\n",
    "\n",
    "# build three Codabench submissions\n",
    "build_and_save_user_specific_submission(recs_grouping, test_in, item_distance, lambda_profile,  \"profile-v2\")\n",
    "build_and_save_user_specific_submission(recs_grouping, test_in, item_distance, lambda_genre,    \"genre-v2\")\n",
    "build_and_save_user_specific_submission(recs_grouping, test_in, item_distance, lambda_combined, \"combined-v2\")"
   ],
   "id": "a7380f3d17767148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ----- hard coded metrics -----\n",
    "baseline = {\n",
    "    \"novelty\": 0.7571858344680189,\n",
    "    \"ndcg\":   0.24037183269526935,\n",
    "}\n",
    "\n",
    "global_points = [\n",
    "    {\"lambda\": 0.0, \"novelty\": 0.7571858344680189, \"ndcg\": 0.24037183269526935},\n",
    "    {\"lambda\": 0.1, \"novelty\": 0.7675673744963164, \"ndcg\": 0.234885227845009},\n",
    "    {\"lambda\": 0.2, \"novelty\": 0.776181917316719,  \"ndcg\": 0.2269467830913513},\n",
    "    {\"lambda\": 0.3, \"novelty\": 0.7829369966718916, \"ndcg\": 0.21763760094619242},\n",
    "    {\"lambda\": 0.4, \"novelty\": 0.7885775842741768, \"ndcg\": 0.2089873200319885},\n",
    "    {\"lambda\": 0.5, \"novelty\": 0.7933494348188609, \"ndcg\": 0.20011688373326467},\n",
    "]\n",
    "\n",
    "user_points = [\n",
    "    {\n",
    "        \"label\": \"User profile novelty\",\n",
    "        \"novelty\": 0.7688015160867505,\n",
    "        \"ndcg\":   0.23173826062697006,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"User genre novelty\",\n",
    "        \"novelty\": 0.7718773808736236,\n",
    "        \"ndcg\":   0.22984383937047892,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"User combined novelty\",\n",
    "        \"novelty\": 0.7702365958206725,\n",
    "        \"ndcg\":   0.23117905870278926,\n",
    "    },\n",
    "]\n",
    "\n",
    "# ----- turn into DataFrames for convenience -----\n",
    "df_global = pd.DataFrame(global_points)\n",
    "df_user   = pd.DataFrame(user_points)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# 1) baseline point\n",
    "plt.scatter(\n",
    "    baseline[\"novelty\"],\n",
    "    baseline[\"ndcg\"],\n",
    "    color=\"black\",\n",
    "    marker=\"*\",\n",
    "    s=140,\n",
    "    label=\"Baseline EASE (λ = 300)\",\n",
    ")\n",
    "\n",
    "# 2) global novelty sweep as a line with points\n",
    "plt.plot(\n",
    "    df_global[\"novelty\"],\n",
    "    df_global[\"ndcg\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"tab:blue\",\n",
    "    label=\"Global novelty weights\",\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# label lambda values near the global points (short labels)\n",
    "for nov, nd, lam in zip(df_global[\"novelty\"], df_global[\"ndcg\"], df_global[\"lambda\"]):\n",
    "    plt.annotate(\n",
    "        f\"{lam}\",\n",
    "        (nov, nd),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(3, 3),\n",
    "        fontsize=8,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "\n",
    "# 3) user specific methods as separate colored markers\n",
    "plt.scatter(\n",
    "    df_user[\"novelty\"],\n",
    "    df_user[\"ndcg\"],\n",
    "    color=\"tab:orange\",\n",
    "    marker=\"D\",\n",
    "    s=70,\n",
    "    label=\"User specific groups\",\n",
    ")\n",
    "\n",
    "# optional: short labels for the three user methods\n",
    "for _, row in df_user.iterrows():\n",
    "    short = row[\"label\"].replace(\"User \", \"\").replace(\" novelty\", \"\")\n",
    "    plt.annotate(\n",
    "        short,  # \"profile\", \"genre\", \"combined\"\n",
    "        (row[\"novelty\"], row[\"ndcg\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(4, 2),\n",
    "        fontsize=8,\n",
    "        color=\"tab:orange\",\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Novelty\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "plt.title(\"Accuracy vs Novelty: baseline, global novelty, user specific groups\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "68903473c6ada5df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a7c6aed39cd4989b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
