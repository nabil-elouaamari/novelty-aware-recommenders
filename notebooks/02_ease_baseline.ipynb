{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.data.loader import load_interactions, load_games\n",
    "from src.evaluation.splitter import split_train_in_out\n",
    "from src.evaluation.evaluator import evaluate_model\n",
    "from src.models.ease import EASE\n",
    "from src.novelty.distance import (\n",
    "    build_genre_similarity_matrix,\n",
    "    build_genre_distance_matrix,\n",
    ")\n",
    "import numpy as np\n",
    "from src.pipelines.save import save_submission\n",
    "from src.config import LAMBDA_REG, TOP_K, N_EVAL_USERS, SEED"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PART 1: OFFLINE EVALUATION",
   "id": "ba484dffd0d00a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load data\n",
    "train = load_interactions(train=True)\n",
    "games = load_games()"
   ],
   "id": "d4a85604f749c478",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Local split (Codabench-like: 1 holdout per user)\n",
    "train_in_full, train_out_full = split_train_in_out(train, seed=SEED)"
   ],
   "id": "b0e3e4fb09c2b0fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Sample a subset of users for faster offline eval\n",
    "all_users = train_out_full[\"user_id\"].unique()          # only users with a holdout\n",
    "rng = np.random.default_rng(SEED)\n",
    "n_eval = min(N_EVAL_USERS, len(all_users))\n",
    "print(\"Total users:\", len(all_users), \"-> Subsampling\", n_eval, \"users...\")\n",
    "sample_users = rng.choice(all_users, size=n_eval, replace=False)"
   ],
   "id": "a9b6c45f4fe0fab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_in = train_in_full[train_in_full[\"user_id\"].isin(sample_users)].reset_index(drop=True)\n",
    "train_out = train_out_full[train_out_full[\"user_id\"].isin(sample_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Offline eval on {n_eval} users \"\n",
    "      f\"(train_in rows = {len(train_in)}, train_out rows = {len(train_out)})\")"
   ],
   "id": "2f1b12c75573861",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5. Fit EASE on *fold-in* for these users and get recommendations\n",
    "model = EASE(lambda_reg=float(LAMBDA_REG))\n",
    "\n",
    "# train_in = history (fold-in) for sampled users\n",
    "# test_in = same df; we score these users\n",
    "recs_offline = model.recommend(\n",
    "    train_in,\n",
    "    train_in,\n",
    "    top_k=TOP_K,\n",
    ")"
   ],
   "id": "7928d29d4a1b153e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Evaluate offline baseline on the sampled users\n",
    "metrics_offline = evaluate_model(\n",
    "    recs_offline,\n",
    "    train_in,\n",
    "    train_out,\n",
    "\n",
    "    # maps item_id -> publisher (needed for publisher Gini)\n",
    "    publisher_mapper=games.set_index(\"item_id\")[\"publisher\"],\n",
    "    item_similarity=build_genre_similarity_matrix(games),\n",
    "    item_distance=build_genre_distance_matrix(games),\n",
    ")\n",
    "print(f\"Offline EASE baseline (lambda_reg = {LAMBDA_REG}) on {n_eval} users\")\n",
    "print(metrics_offline)"
   ],
   "id": "9df5ea2fe9d36ba6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PART 2: ONLINE EVALUATION (codabench)",
   "id": "c97c97396a422a5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load full training and official test fold-in\n",
    "train_full = load_interactions(train=True)\n",
    "test_in = load_interactions(train=False)"
   ],
   "id": "436a3a1fb22a7b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. New model instance, same lambda\n",
    "model_cb = EASE(lambda_reg=float(LAMBDA_REG))"
   ],
   "id": "b6c8f63f56b76f41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. train_full is used to fit; test_in defines which users we score\n",
    "recs_codabench = model_cb.recommend(train_full, test_in, top_k=TOP_K)"
   ],
   "id": "e352b555b74bd367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Quick sanity check\n",
    "print(recs_codabench.head(n=40))"
   ],
   "id": "6aaa826e040e9e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7. Save submission CSV (this writes submission_name.csv in notebooks or pipelines location)\n",
    "submission_name = f\"ease_baseline_lambda_{LAMBDA_REG}_final\"\n",
    "save_submission(recs_codabench, submission_name)\n",
    "print(f\"Saved Codabench submission as {submission_name}.csv\")"
   ],
   "id": "a4a331b8f21712e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "344aa71deaec76bd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
